{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b09bd8a-9d3a-44a8-8dfd-42242fbab1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab42f82-4842-42c6-ae31-92655fb64e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('./Hayao_Miyazaki.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b4386-ec9a-47ba-af35-f2eef2964903",
   "metadata": {},
   "source": [
    "chunk_size=1000: Specifies the maximum number of characters in each chunk.\n",
    "\n",
    "chunk_overlap=4: Specifies the number of characters that will overlap between consecutive chunks. This helps ensure that information is not lost between chunks.\n",
    "\n",
    "The HuggingFaceEmbeddings class is likely part of a library that provides embeddings, which are numerical representations of text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b9e625-554c-4066-976e-c1d2a0dc841a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 3293, which is longer than the specified 1000\n",
      "Created a chunk of size 1255, which is longer than the specified 1000\n",
      "Created a chunk of size 1103, which is longer than the specified 1000\n",
      "Created a chunk of size 1517, which is longer than the specified 1000\n",
      "Created a chunk of size 1699, which is longer than the specified 1000\n",
      "Created a chunk of size 1585, which is longer than the specified 1000\n",
      "Created a chunk of size 1336, which is longer than the specified 1000\n",
      "Created a chunk of size 1020, which is longer than the specified 1000\n",
      "Created a chunk of size 1241, which is longer than the specified 1000\n",
      "Created a chunk of size 1538, which is longer than the specified 1000\n",
      "Created a chunk of size 1899, which is longer than the specified 1000\n",
      "Created a chunk of size 1177, which is longer than the specified 1000\n",
      "Created a chunk of size 1763, which is longer than the specified 1000\n",
      "Created a chunk of size 1509, which is longer than the specified 1000\n",
      "Created a chunk of size 1401, which is longer than the specified 1000\n",
      "Created a chunk of size 1234, which is longer than the specified 1000\n",
      "Created a chunk of size 1093, which is longer than the specified 1000\n",
      "Created a chunk of size 2614, which is longer than the specified 1000\n",
      "Created a chunk of size 1158, which is longer than the specified 1000\n",
      "Created a chunk of size 1331, which is longer than the specified 1000\n",
      "Created a chunk of size 1315, which is longer than the specified 1000\n",
      "Created a chunk of size 1133, which is longer than the specified 1000\n",
      "Created a chunk of size 1131, which is longer than the specified 1000\n",
      "Created a chunk of size 1413, which is longer than the specified 1000\n",
      "Created a chunk of size 1097, which is longer than the specified 1000\n",
      "Created a chunk of size 3359, which is longer than the specified 1000\n",
      "Created a chunk of size 77762, which is longer than the specified 1000\n",
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=4)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d4fa90-f52b-4346-8d0c-099671f7eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f9d2b9-f6f8-466f-9561-5bf4521cc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api_key = os.getenv('PINECONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12080b-7e81-43a1-a566-1d545c6fb75e",
   "metadata": {},
   "source": [
    "Pinecone is a vector db to store the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bee14f7d-e45f-419a-bd81-0feeca8a1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone client\n",
    "pinecone.init(\n",
    "    api_key= pinecone_api_key,\n",
    "    environment='gcp-starter'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e349232-8606-494e-9e72-ef8843a23839",
   "metadata": {},
   "source": [
    "We store the embeddings in Pinecone in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eadc7676-4992-4403-9cc6-a5d9c0f2502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Index Name\n",
    "index_name = \"langchain-demo\"\n",
    "\n",
    "# Checking Index\n",
    "if index_name not in pinecone.list_indexes():\n",
    "  # Create new Index\n",
    "  pinecone.create_index(name=index_name, metric=\"cosine\", dimension=768)\n",
    "  docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "else:\n",
    "  # Link to the existing index\n",
    "  docsearch = Pinecone.from_existing_index(index_name, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec02965-f4c9-42df-8f16-eb131bc5d823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.pinecone.Pinecone at 0x78e9272c5960>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0b62e0b-20ac-444c-9310-7b9eeb61f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_key = os.getenv('HF_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3068cb34-1a9c-4dd4-b540-95c663d40f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# Define the repo ID and connect to Mixtral model on Huggingface\n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "  repo_id=model, \n",
    "  model_kwargs={\"temperature\": 0.8, \"top_k\": 50}, \n",
    "  huggingfacehub_api_token=hf_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b6fbb94-a11c-4c2b-823c-45309b4608e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4564d-f884-4ad1-ae32-969950dcca91",
   "metadata": {},
   "source": [
    "template:\n",
    "- Instruction\n",
    "- Context Placeholder: {context}\n",
    "- Question Placeholder: {question}\n",
    "- Answer Prompt\n",
    "\n",
    "The placeholders {context} and {question} in the template will be dynamically replaced with actual values provided when generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8673b50a-7949-477f-bfad-f1dd49607875",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"\n",
    "The Human will ask questions about Hayao Miyazaki life. \n",
    "If you don't know the answer, just say you don't know. \n",
    "Keep the answer 2-3 sentences.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template=template, \n",
    "  input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "073f616d-c8ad-40fa-83c5-b9b2e0a32f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f02038-2434-4293-8957-a096b532f51f",
   "metadata": {},
   "source": [
    "RunnablePassthrough(): the question input through without modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e9af32f-68e4-4d59-bc72-3035c8d58450",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "{\"context\": docsearch.as_retriever(),  \"question\": RunnablePassthrough()} \n",
    "| prompt \n",
    "| llm\n",
    "| StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c247f31-c55c-40e6-b40c-868899a5a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup for the retriever, question, prompt, LLM, and output parser\n",
    "# retriever = docsearch.as_retriever()\n",
    "\n",
    "# # Create the PromptTemplate object\n",
    "# prompt = PromptTemplate(\n",
    "#     template=template, \n",
    "#     input_variables=[\"context\", \"question\"]\n",
    "# )\n",
    "\n",
    "# # Define a function to handle the chatbot response generation\n",
    "# def generate_response(user_question):\n",
    "#     # Retrieve the context \n",
    "#     context = retriever.get_relevant_documents(user_question)\n",
    "\n",
    "#     # Format the prompt using the provided context and user question\n",
    "#     formatted_prompt = prompt.format(context=context, question=user_question)\n",
    "\n",
    "#     # Generate the response using the language model\n",
    "#     llm_response = llm(formatted_prompt)\n",
    "\n",
    "#     # Parse the response to get the final string output\n",
    "#     parsed_output = StrOutputParser().parse(llm_response)\n",
    "\n",
    "#     return parsed_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e634435-46f7-45ee-b0aa-b2cc39091582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# user_question = input(\"Ask me anything: \")\n",
    "# response = generate_response(user_question)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa985960-ac28-4ff8-bbfa-f01b29252c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask me anything:  how is miyazaki?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Human will ask questions about Hayao Miyazaki life. \n",
      "If you don't know the answer, just say you don't know. \n",
      "Keep the answer 2-3 sentences.\n",
      "\n",
      "Context: []\n",
      "Question: how is miyazaki?\n",
      "Answer: \n",
      "\n",
      "I don't have real-time information about Hayao Miyazaki's current state. As of the last update, he is enjoying his retirement, spending time with his family, and continuing to work on smaller animation projects.\n"
     ]
    }
   ],
   "source": [
    "user_input = input(\"Ask me anything: \")\n",
    "result = rag_chain.invoke(user_input)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727dd99-42c9-4c3e-9e32-dbd281dc28ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
